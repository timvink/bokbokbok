{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the bokbokbok doks! \u00b6 bokbokbok is a Python library that lets us easily implement custom loss functions and eval metrics in LightGBM and XGBoost. Installation \u00b6 In order to install bokbokbok you need to use Python 3.6 or higher. Install bokbokbok via pip with: pip install bokbokbok Alternatively you can fork/clone and run: git clone https://gitlab.com/orchardbirds/bokbokbok.git cd bokbokbok pip install . Licence \u00b6 todo","title":"Home"},{"location":"index.html#welcome-to-the-bokbokbok-doks","text":"bokbokbok is a Python library that lets us easily implement custom loss functions and eval metrics in LightGBM and XGBoost.","title":"Welcome to the bokbokbok doks!"},{"location":"index.html#installation","text":"In order to install bokbokbok you need to use Python 3.6 or higher. Install bokbokbok via pip with: pip install bokbokbok Alternatively you can fork/clone and run: git clone https://gitlab.com/orchardbirds/bokbokbok.git cd bokbokbok pip install .","title":"Installation"},{"location":"index.html#licence","text":"todo","title":"Licence"},{"location":"derivations/focal.html","text":"Weighted Focal Loss \u00b6 Weighted Focal Loss applies a scaling parameter $\\alpha$ and a focusing parameter $\\gamma$ to Binary Cross Entropy We take the definition of the Focal Loss from this paper : \\begin{equation} L_{FL} = - \\alpha(1 - p_{t})^{ \\gamma} \\log p_{t} \\end{equation} where: \\begin{equation} p_{t}= \\begin{cases} \\hat{y},& \\text{if } y = 1\\ 1 - \\hat{y}, & \\text{otherwise} \\end{cases} \\end{equation} This is equivalent to writing: \\begin{equation} L_{FL} = - \\alpha y(1 - \\hat{y})^{\\gamma} \\log \\hat{y} - (1 - y)\\hat{y}^{\\gamma} \\log(1 - \\hat{y}) \\end{equation} We calculate the Gradient: \\begin{aligned} G_{FL}(z) & = \\frac{\\partial L_{FL}}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (\\alpha \\gamma y (1 - \\hat{y})^{\\gamma - 1} \\log\\hat{y} \\ & - \\alpha\\frac{y}{\\hat{y}}(1 - \\hat{y}) \\ & - \\gamma (1 - y)\\hat{y}^{\\gamma - 1} \\log(1 - \\hat{y}) \\ & + \\frac{1 - y}{1 - \\hat{y}}\\hat{y}^{\\gamma})\\ & \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = \\alpha y(1 - \\hat{y})^{\\gamma}(\\gamma\\hat{y} \\log \\hat{y} + \\hat{y} - 1) + (1 - y) \\hat{y}^{\\gamma}(\\gamma(\\hat{y} - 1)\\log(1 - \\hat{y}) + \\hat{y}) \\end{aligned} and Hessian: \\begin{aligned} H_{FL}(z) & = \\frac{\\partial G_{FL}(z)}{\\partial z} = \\frac{\\partial G_{FL}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial G_{FL}(z)}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (\\alpha\\gamma y (1 - \\hat{y})^{\\gamma}\\log\\hat{y} \\ & + \\alpha\\gamma y (1 - \\hat{y})^{\\gamma} \\ & - \\alpha\\gamma^{2}y\\hat{y}(1 - \\hat{y})^{\\gamma - 1}\\log\\hat{y} \\ & + \\alpha y (\\gamma + 1)(1 - \\hat{y})^{\\gamma} \\ & - \\gamma^{2}(1 - y)\\hat{y}^{\\gamma - 1}\\log(1 - \\hat{y})(1 - \\hat{y}) \\ & + \\gamma(1 - y)\\hat{y}^{\\gamma} \\ & + \\gamma(1 - y)\\hat{y}^{\\gamma}\\log(1 - \\hat{y}) \\ & + (\\gamma + 1)(1 - y)\\hat{y}^{\\gamma}) \\ & \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = \\alpha y\\hat{y}(1 - y)^{\\gamma}(\\gamma(1 - \\hat{y})\\log\\hat{y} + 2\\gamma(1 - \\hat{y}) - \\gamma^{2}\\hat{y}\\log\\hat{y} + 1 - \\hat{y}) \\ & + (1 - y)\\hat{y}^{\\gamma + 1}(1 - \\hat{y})(2\\gamma + \\gamma(\\log(1-\\hat{y})) + 1) \\end{aligned} By setting $\\alpha = 1$ and $\\gamma = 0$ we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Focal Loss"},{"location":"derivations/focal.html#weighted-focal-loss","text":"Weighted Focal Loss applies a scaling parameter $\\alpha$ and a focusing parameter $\\gamma$ to Binary Cross Entropy We take the definition of the Focal Loss from this paper : \\begin{equation} L_{FL} = - \\alpha(1 - p_{t})^{ \\gamma} \\log p_{t} \\end{equation} where: \\begin{equation} p_{t}= \\begin{cases} \\hat{y},& \\text{if } y = 1\\ 1 - \\hat{y}, & \\text{otherwise} \\end{cases} \\end{equation} This is equivalent to writing: \\begin{equation} L_{FL} = - \\alpha y(1 - \\hat{y})^{\\gamma} \\log \\hat{y} - (1 - y)\\hat{y}^{\\gamma} \\log(1 - \\hat{y}) \\end{equation} We calculate the Gradient: \\begin{aligned} G_{FL}(z) & = \\frac{\\partial L_{FL}}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (\\alpha \\gamma y (1 - \\hat{y})^{\\gamma - 1} \\log\\hat{y} \\ & - \\alpha\\frac{y}{\\hat{y}}(1 - \\hat{y}) \\ & - \\gamma (1 - y)\\hat{y}^{\\gamma - 1} \\log(1 - \\hat{y}) \\ & + \\frac{1 - y}{1 - \\hat{y}}\\hat{y}^{\\gamma})\\ & \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = \\alpha y(1 - \\hat{y})^{\\gamma}(\\gamma\\hat{y} \\log \\hat{y} + \\hat{y} - 1) + (1 - y) \\hat{y}^{\\gamma}(\\gamma(\\hat{y} - 1)\\log(1 - \\hat{y}) + \\hat{y}) \\end{aligned} and Hessian: \\begin{aligned} H_{FL}(z) & = \\frac{\\partial G_{FL}(z)}{\\partial z} = \\frac{\\partial G_{FL}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial G_{FL}(z)}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (\\alpha\\gamma y (1 - \\hat{y})^{\\gamma}\\log\\hat{y} \\ & + \\alpha\\gamma y (1 - \\hat{y})^{\\gamma} \\ & - \\alpha\\gamma^{2}y\\hat{y}(1 - \\hat{y})^{\\gamma - 1}\\log\\hat{y} \\ & + \\alpha y (\\gamma + 1)(1 - \\hat{y})^{\\gamma} \\ & - \\gamma^{2}(1 - y)\\hat{y}^{\\gamma - 1}\\log(1 - \\hat{y})(1 - \\hat{y}) \\ & + \\gamma(1 - y)\\hat{y}^{\\gamma} \\ & + \\gamma(1 - y)\\hat{y}^{\\gamma}\\log(1 - \\hat{y}) \\ & + (\\gamma + 1)(1 - y)\\hat{y}^{\\gamma}) \\ & \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = \\alpha y\\hat{y}(1 - y)^{\\gamma}(\\gamma(1 - \\hat{y})\\log\\hat{y} + 2\\gamma(1 - \\hat{y}) - \\gamma^{2}\\hat{y}\\log\\hat{y} + 1 - \\hat{y}) \\ & + (1 - y)\\hat{y}^{\\gamma + 1}(1 - \\hat{y})(2\\gamma + \\gamma(\\log(1-\\hat{y})) + 1) \\end{aligned} By setting $\\alpha = 1$ and $\\gamma = 0$ we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Focal Loss"},{"location":"derivations/note.html","text":"A Note About Gradients in Classification Problems \u00b6 For the gradient boosting packages we have to calculate the gradient of the Loss function with respect to the marginal probabilites . In this case, we must calculate \\begin{aligned} \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\end{aligned} The Hessian is similarly calculated: \\begin{aligned} \\frac{\\partial^{2} L}{\\partial z^{2}} = \\frac{\\partial}{\\partial z}[\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}] \\end{aligned} Where $\\hat{y}$ is the sigmoid function, unless stated otherwise : \\begin{aligned} \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\end{aligned} We will make use of the following property for the calculations of the Losses and Hessians: \\begin{aligned} \\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y} \\cdot (1 - \\hat{y}) \\end{aligned}","title":"General Remarks"},{"location":"derivations/note.html#a-note-about-gradients-in-classification-problems","text":"For the gradient boosting packages we have to calculate the gradient of the Loss function with respect to the marginal probabilites . In this case, we must calculate \\begin{aligned} \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\end{aligned} The Hessian is similarly calculated: \\begin{aligned} \\frac{\\partial^{2} L}{\\partial z^{2}} = \\frac{\\partial}{\\partial z}[\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}] \\end{aligned} Where $\\hat{y}$ is the sigmoid function, unless stated otherwise : \\begin{aligned} \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\end{aligned} We will make use of the following property for the calculations of the Losses and Hessians: \\begin{aligned} \\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y} \\cdot (1 - \\hat{y}) \\end{aligned}","title":"A Note About Gradients in Classification Problems"},{"location":"derivations/wce.html","text":"Weighted Cross Entropy Loss \u00b6 Weighted Cross Entropy applies a scaling parameter $\\alpha$ to Binary Cross Entropy , allowing us to penalise false positives or false negatives more harshly. If you want false positives to be penalised more than false negatives, $\\alpha$ must be greater than 1. Otherwise, it must be less than 1. The equations for Binary and Weighted Cross Entropy Loss are the following: \\begin{aligned} L_{BCE} = -y \\log(\\hat{y}(z)) - (1 - y)\\log(1 - \\hat{y}(z)) \\ L_{WCE} = - \\alpha y \\log(\\hat{y}(z)) - (1 - y)\\log(1 - \\hat{y}(z)) \\end{aligned} We calculate the Gradient: \\begin{aligned} G_{WCE}(z) & = \\frac{\\partial L_{WCE}}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = [\\frac{-\\alpha y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}]\\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = y\\hat{y}(\\alpha - 1) + \\hat{y} - \\alpha y \\end{aligned} and Hessian: \\begin{aligned} H_{WCE}(z) & = \\frac{\\partial G_{WCE}(z)}{\\partial z} = \\frac{\\partial G_{WCE}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial G_{WCE}(z)}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (y(\\alpha - 1) + 1) \\cdot \\hat{y}(1 - \\hat{y}) \\end{aligned} By setting $\\alpha = 1$ we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Cross Entropy"},{"location":"derivations/wce.html#weighted-cross-entropy-loss","text":"Weighted Cross Entropy applies a scaling parameter $\\alpha$ to Binary Cross Entropy , allowing us to penalise false positives or false negatives more harshly. If you want false positives to be penalised more than false negatives, $\\alpha$ must be greater than 1. Otherwise, it must be less than 1. The equations for Binary and Weighted Cross Entropy Loss are the following: \\begin{aligned} L_{BCE} = -y \\log(\\hat{y}(z)) - (1 - y)\\log(1 - \\hat{y}(z)) \\ L_{WCE} = - \\alpha y \\log(\\hat{y}(z)) - (1 - y)\\log(1 - \\hat{y}(z)) \\end{aligned} We calculate the Gradient: \\begin{aligned} G_{WCE}(z) & = \\frac{\\partial L_{WCE}}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = [\\frac{-\\alpha y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}]\\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = y\\hat{y}(\\alpha - 1) + \\hat{y} - \\alpha y \\end{aligned} and Hessian: \\begin{aligned} H_{WCE}(z) & = \\frac{\\partial G_{WCE}(z)}{\\partial z} = \\frac{\\partial G_{WCE}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\ & = \\frac{\\partial G_{WCE}(z)}{\\partial \\hat{y}} \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\ & = (y(\\alpha - 1) + 1) \\cdot \\hat{y}(1 - \\hat{y}) \\end{aligned} By setting $\\alpha = 1$ we obtain the Gradient and Hessian for Binary Cross Entropy Loss, as expected.","title":"Weighted Cross Entropy Loss"}]}